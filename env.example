# Environment Configuration for Voice Assistant
# Copy this file to .env and customize as needed
#
# This file is automatically loaded by python-dotenv
# No need to export variables manually!

# ============================================================================
# Feature Flags
# ============================================================================

# Enable wake word detection mode (true/false)
# When true: Assistant waits for wake word ("Hey Jarvis") before listening
# When false: Continuous listening mode (always listening)
# Default: false
USE_WAKEWORD=false

# LLM backend to use: ollama (local) or litellm (online API)
# - ollama: Local inference using Ollama server
# - litellm: Online API (OpenAI, Anthropic, etc.) - requires LITELLM_API_KEY
# Default: ollama
LLM_BACKEND=ollama

# Enable internet tools: web search, URL scraping, weather (true/false)
# Requires Firecrawl server running for web tools
# Default: false
USE_TOOLS=false

# Enable memory usage monitoring (true/false)
# Shows periodic memory stats in console
# Default: true
ENABLE_MEMORY_MONITOR=true

# Memory monitor update interval in seconds
# Default: 60
MONITOR_INTERVAL=60

# Enable automatic noise calibration at startup (true/false)
# Calibrates VAD and wakeword thresholds based on ambient noise
# Default: true
AUTO_CALIBRATE=true

# Duration of ambient noise sampling for calibration in seconds
# Default: 2.0
CALIBRATION_SECONDS=2.0

# ============================================================================
# Ollama Configuration (for local/offline versions)
# ============================================================================

# Ollama API URL
# Default: http://localhost:11434/api/chat
OLLAMA_URL=http://localhost:11434/api/chat

# Ollama Model to use
# For basic assistant (main.py): Use gemma3:270m (smallest, no internet)
# For internet-connected (main_internetconnected.py, chat_internetconnected.py): 
#   Use tool-capable models like llama3.2:1b, llama3.1:3b, mistral-nemo
# Default for main.py: gemma3:270m
# Default for internet versions: llama3.2:1b
OLLAMA_MODEL=gemma3:270m

# ============================================================================
# LiteLLM Configuration (for online versions - main_wakeword_online.py)
# ============================================================================

# LiteLLM API URL (OpenAI-compatible endpoint)
# For OpenAI: https://api.openai.com/v1/chat/completions
# For Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT/chat/completions?api-version=2023-05-15
# For other providers: Check your provider's API documentation
# Default: https://api.openai.com/v1/chat/completions
LITELLM_URL=https://api.openai.com/v1/chat/completions

# LiteLLM Model to use
# For OpenAI: gpt-3.5-turbo, gpt-4, gpt-4-turbo, etc.
# For Anthropic: claude-3-opus, claude-3-sonnet, etc.
# For other providers: Check your provider's model names
# Default: gpt-3.5-turbo
LITELLM_MODEL=gpt-3.5-turbo

# LiteLLM API Key
# Required for authentication with the LLM provider
# Get your API key from your provider's dashboard
LITELLM_API_KEY=your_api_key_here

# ============================================================================
# Firecrawl Configuration (for internet-connected versions only)
# ============================================================================

# Firecrawl API URL (self-hosted instance)
# Default: http://localhost:3002
FIRECRAWL_URL=http://localhost:3002

# Firecrawl API Key (leave empty for self-hosted without authentication)
# Default: empty
FIRECRAWL_API_KEY=

# ============================================================================
# OpenWeatherMap Configuration (for weather tool)
# ============================================================================

# OpenWeatherMap API Key (get one free at https://openweathermap.org/api)
# Required for weather functionality
# Default: empty (weather tool will be disabled if not set)
OPENWEATHERMAP_API_KEY=

# Default location for weather queries when no location is specified
# Format: "City, State" or "City, Country" (e.g., "North Plains, OR", "London, UK")
# Default: empty (will ask user for location if not set)
DEFAULT_WEATHER_LOCATION=

# ============================================================================
# Wake Word Configuration (for wakeword-enabled versions only)
# ============================================================================

# Wake Word Model to use
# Available models: hey_jarvis_v0.1, alexa_v0.1, hey_mycroft_v0.1, hey_rhasspy_v0.1
# Default: hey_jarvis_v0.1
WAKEWORD_MODEL=hey_jarvis_v0.1

# Wake Word Detection Threshold (0.0 to 1.0)
# Lower = more sensitive (more false positives)
# Higher = less sensitive (may miss activations)
# Default: 0.5
WAKEWORD_THRESHOLD=0.5

# ============================================================================
# VAD (Voice Activity Detection) Configuration
# ============================================================================

# Enable VAD (Voice Activity Detection) for automatic speech detection (true/false)
# When true: Uses WebRTC VAD to detect when you start/stop speaking
# When false: Uses fixed-duration recording (see FIXED_LISTEN_SECONDS)
# Default: true
USE_VAD=true

# Fixed listening duration in seconds (only used when USE_VAD=false)
# How long to record audio before sending to STT
# Tip: Set to expected average speech duration (3-10 seconds typical)
# Default: 5.0
FIXED_LISTEN_SECONDS=5.0

# VAD sample rate in Hz
# Default: 16000 (recommended for speech recognition)
VAD_SAMPLE_RATE=16000

# VAD frame duration in milliseconds
# Must be 10, 20, or 30 ms (WebRTC VAD requirement)
# Default: 30
VAD_FRAME_DURATION_MS=30

# VAD padding duration in milliseconds
# How long to continue recording after speech stops
# Higher = captures more trailing audio (good for slower speakers)
# Default: 360
VAD_PADDING_DURATION_MS=360

# VAD aggressiveness level (0-3)
# 0 = Least aggressive (most sensitive, may pick up background noise)
# 1 = Low aggressiveness (good for quiet rooms)
# 2 = Moderate aggressiveness (balanced, default)
# 3 = Most aggressive (best for noisy environments, may miss soft speech)
# Note: Auto-calibration may override this based on ambient noise
# Default: 2
VAD_AGGRESSIVENESS=2

# VAD activation ratio (0.0-1.0)
# Fraction of frames in buffer that must be voiced to trigger speech start
# Lower = triggers speech detection faster
# Default: 0.6
VAD_ACTIVATION_RATIO=0.6

# VAD deactivation ratio (0.0-1.0)
# Fraction of frames in buffer that must be unvoiced to end speech
# Higher = waits longer before ending speech segment
# Default: 0.85
VAD_DEACTIVATION_RATIO=0.85

# ============================================================================
# Usage Examples
# ============================================================================

# Example 1: Basic voice assistant with default settings
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=gemma3:270m

# Example 2: Internet-connected with Llama 3.1 8B
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://localhost:3002

# Example 3: Remote Ollama server
# OLLAMA_URL=http://192.168.1.100:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b

# Example 4: Memory-constrained setup with smaller model
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.1:3b
# FIRECRAWL_URL=http://localhost:3002

# Example 5: Remote Firecrawl instance
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://192.168.1.200:3002

# Example 6: Online wake word assistant with OpenAI (main_wakeword_online.py)
# LITELLM_URL=https://api.openai.com/v1/chat/completions
# LITELLM_MODEL=gpt-3.5-turbo
# LITELLM_API_KEY=sk-...your_key_here...
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# Example 7: Online wake word assistant with Claude via Anthropic API
# LITELLM_URL=https://api.anthropic.com/v1/messages
# LITELLM_MODEL=claude-3-sonnet-20240229
# LITELLM_API_KEY=sk-ant-...your_key_here...
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# Example 8: Online internet-connected assistant with tools (main_wakeword_internetconnected_online.py)
# LITELLM_URL=https://api.openai.com/v1/chat/completions
# LITELLM_MODEL=gpt-4
# LITELLM_API_KEY=sk-...your_key_here...
# FIRECRAWL_URL=http://localhost:3002
# OPENWEATHERMAP_API_KEY=your_openweathermap_key
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# Example 9: High-quality TTS with Chatterbox GPU + OpenAI Whisper STT
# STT_ENGINE=openai-whisper
# TTS_ENGINE=chatterbox
# TTS_GPU=true
# TTS_VOICE=morgan_freeman

# ============================================================================
# STT (Speech-to-Text) Engine Configuration
# ============================================================================

# Default STT engine to use
# Options: faster-whisper, openai-whisper
# - faster-whisper: Uses ctranslate2, faster but may have cuDNN issues on RTX 50 series
# - openai-whisper: Uses PyTorch, better GPU compatibility (recommended for RTX 5090)
# Default: faster-whisper
STT_ENGINE=faster-whisper

# Enable GPU for STT (true/false)
# Default: false (auto-detects if not set)
STT_GPU=false

# Device for STT (cpu or cuda). Overrides STT_GPU if set.
# STT_DEVICE=

# ============================================================================
# TTS (Text-to-Speech) Engine Configuration
# ============================================================================

# Default TTS engine to use
# Options: kokoro, piper, chatterbox, vibevoice
# - kokoro: Fast CPU-based ONNX TTS
# - piper: Lightweight local TTS  
# - chatterbox: Zero-shot voice cloning with GPU support (recommended for quality)
# - vibevoice: Neural TTS with multiple speakers
# Default: kokoro
TTS_ENGINE=kokoro

# Enable GPU for TTS (true/false)
# Recommended for chatterbox and vibevoice
# Default: false
TTS_GPU=false

# Default voice character for Chatterbox TTS
# Uses voices/{name}.wav for voice cloning and personas/{name}.txt for personality
# Available: jamaican, morgan_freeman, snoop_dogg, jerry_seinfeld, aave_female, etc.
# TTS_VOICE=

# ============================================================================
# Notes
# ============================================================================

# Tool-capable models for internet features (Ollama - local):
# - llama3.2:1b (recommended, ~5GB RAM)
# - llama3.1:3b (smaller, ~2-3GB RAM)
# - mistral-nemo
# - firefunction-v2
# - command-r-plus

# Tool-capable models for online services (OpenAI, Anthropic, etc.):
# - OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo (all support function calling)
# - Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
# - Note: Tool support varies by provider and model. Check your provider's documentation.

# Non-tool models for basic assistant (Ollama - local):
# - gemma3:270m (smallest, 270MB)
# - qwen2.5:0.5b (397MB)
# - qwen2.5:1.5b (986MB)
# - llama3.2:1b (1.3GB)

# To install Ollama models: ollama pull <model-name>
# To list installed Ollama models: ollama ls

