# Environment Configuration for Voice Assistant
# Copy this file to .env and customize as needed
#
# This file is automatically loaded by python-dotenv
# No need to export variables manually!

# ============================================================================
# Ollama Configuration (for local/offline versions)
# ============================================================================

# Ollama API URL
# Default: http://localhost:11434/api/chat
OLLAMA_URL=http://localhost:11434/api/chat

# Ollama Model to use
# For basic assistant (main.py): Use gemma3:270m (smallest, no internet)
# For internet-connected (main_internetconnected.py, chat_internetconnected.py): 
#   Use tool-capable models like llama3.2:1b, llama3.1:3b, mistral-nemo
# Default for main.py: gemma3:270m
# Default for internet versions: llama3.2:1b
OLLAMA_MODEL=gemma3:270m

# ============================================================================
# LiteLLM Configuration (for online versions - main_wakeword_online.py)
# ============================================================================

# LiteLLM API URL (OpenAI-compatible endpoint)
# For OpenAI: https://api.openai.com/v1/chat/completions
# For Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT/chat/completions?api-version=2023-05-15
# For other providers: Check your provider's API documentation
# Default: https://api.openai.com/v1/chat/completions
LITELLM_URL=https://api.openai.com/v1/chat/completions

# LiteLLM Model to use
# For OpenAI: gpt-3.5-turbo, gpt-4, gpt-4-turbo, etc.
# For Anthropic: claude-3-opus, claude-3-sonnet, etc.
# For other providers: Check your provider's model names
# Default: gpt-3.5-turbo
LITELLM_MODEL=gpt-3.5-turbo

# LiteLLM API Key
# Required for authentication with the LLM provider
# Get your API key from your provider's dashboard
LITELLM_API_KEY=your_api_key_here

# ============================================================================
# Firecrawl Configuration (for internet-connected versions only)
# ============================================================================

# Firecrawl API URL (self-hosted instance)
# Default: http://localhost:3002
FIRECRAWL_URL=http://localhost:3002

# Firecrawl API Key (leave empty for self-hosted without authentication)
# Default: empty
FIRECRAWL_API_KEY=

# ============================================================================
# OpenWeatherMap Configuration (for weather tool)
# ============================================================================

# OpenWeatherMap API Key (get one free at https://openweathermap.org/api)
# Required for weather functionality
# Default: empty (weather tool will be disabled if not set)
OPENWEATHERMAP_API_KEY=

# ============================================================================
# Wake Word Configuration (for wakeword-enabled versions only)
# ============================================================================

# Wake Word Model to use
# Available models: hey_jarvis_v0.1, alexa_v0.1, hey_mycroft_v0.1, hey_rhasspy_v0.1
# Default: hey_jarvis_v0.1
WAKEWORD_MODEL=hey_jarvis_v0.1

# Wake Word Detection Threshold (0.0 to 1.0)
# Lower = more sensitive (more false positives)
# Higher = less sensitive (may miss activations)
# Default: 0.5
WAKEWORD_THRESHOLD=0.5

# ============================================================================
# Usage Examples
# ============================================================================

# Example 1: Basic voice assistant with default settings
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=gemma3:270m

# Example 2: Internet-connected with Llama 3.1 8B
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://localhost:3002

# Example 3: Remote Ollama server
# OLLAMA_URL=http://192.168.1.100:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b

# Example 4: Memory-constrained setup with smaller model
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.1:3b
# FIRECRAWL_URL=http://localhost:3002

# Example 5: Remote Firecrawl instance
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://192.168.1.200:3002

# Example 6: Online wake word assistant with OpenAI (main_wakeword_online.py)
# LITELLM_URL=https://api.openai.com/v1/chat/completions
# LITELLM_MODEL=gpt-3.5-turbo
# LITELLM_API_KEY=sk-...your_key_here...
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# Example 7: Online wake word assistant with Claude via Anthropic API
# LITELLM_URL=https://api.anthropic.com/v1/messages
# LITELLM_MODEL=claude-3-sonnet-20240229
# LITELLM_API_KEY=sk-ant-...your_key_here...
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# Example 8: Online internet-connected assistant with tools (main_wakeword_internetconnected_online.py)
# LITELLM_URL=https://api.openai.com/v1/chat/completions
# LITELLM_MODEL=gpt-4
# LITELLM_API_KEY=sk-...your_key_here...
# FIRECRAWL_URL=http://localhost:3002
# OPENWEATHERMAP_API_KEY=your_openweathermap_key
# WAKEWORD_MODEL=hey_jarvis_v0.1
# WAKEWORD_THRESHOLD=0.5

# ============================================================================
# Notes
# ============================================================================

# Tool-capable models for internet features (Ollama - local):
# - llama3.2:1b (recommended, ~5GB RAM)
# - llama3.1:3b (smaller, ~2-3GB RAM)
# - mistral-nemo
# - firefunction-v2
# - command-r-plus

# Tool-capable models for online services (OpenAI, Anthropic, etc.):
# - OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo (all support function calling)
# - Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
# - Note: Tool support varies by provider and model. Check your provider's documentation.

# Non-tool models for basic assistant (Ollama - local):
# - gemma3:270m (smallest, 270MB)
# - qwen2.5:0.5b (397MB)
# - qwen2.5:1.5b (986MB)
# - llama3.2:1b (1.3GB)

# To install Ollama models: ollama pull <model-name>
# To list installed Ollama models: ollama ls

