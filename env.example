# Environment Configuration for Voice Assistant
# Copy this file to .env and customize as needed
#
# This file is automatically loaded by python-dotenv
# No need to export variables manually!

# ============================================================================
# Ollama Configuration
# ============================================================================

# Ollama API URL
# Default: http://localhost:11434/api/chat
OLLAMA_URL=http://localhost:11434/api/chat

# Ollama Model to use
# For basic assistant (main.py): Use gemma3:270m (smallest, no internet)
# For internet-connected (main_internetconnected.py, chat_internetconnected.py): 
#   Use tool-capable models like llama3.2:1b, llama3.1:3b, mistral-nemo
# Default for main.py: gemma3:270m
# Default for internet versions: llama3.2:1b
OLLAMA_MODEL=gemma3:270m

# ============================================================================
# Firecrawl Configuration (for internet-connected versions only)
# ============================================================================

# Firecrawl API URL (self-hosted instance)
# Default: http://localhost:3002
FIRECRAWL_URL=http://localhost:3002

# Firecrawl API Key (leave empty for self-hosted without authentication)
# Default: empty
FIRECRAWL_API_KEY=

# ============================================================================
# OpenWeatherMap Configuration (for weather tool)
# ============================================================================

# OpenWeatherMap API Key (get one free at https://openweathermap.org/api)
# Required for weather functionality
# Default: empty (weather tool will be disabled if not set)
OPENWEATHERMAP_API_KEY=

# ============================================================================
# Usage Examples
# ============================================================================

# Example 1: Basic voice assistant with default settings
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=gemma3:270m

# Example 2: Internet-connected with Llama 3.1 8B
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://localhost:3002

# Example 3: Remote Ollama server
# OLLAMA_URL=http://192.168.1.100:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b

# Example 4: Memory-constrained setup with smaller model
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.1:3b
# FIRECRAWL_URL=http://localhost:3002

# Example 5: Remote Firecrawl instance
# OLLAMA_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3.2:1b
# FIRECRAWL_URL=http://192.168.1.200:3002

# ============================================================================
# Notes
# ============================================================================

# Tool-capable models for internet features:
# - llama3.2:1b (recommended, ~5GB RAM)
# - llama3.1:3b (smaller, ~2-3GB RAM)
# - mistral-nemo
# - firefunction-v2
# - command-r-plus

# Non-tool models for basic assistant:
# - gemma3:270m (smallest, 270MB)
# - qwen2.5:0.5b (397MB)
# - qwen2.5:1.5b (986MB)
# - llama3.2:1b (1.3GB)

# To install models: ollama pull <model-name>
# To list installed models: ollama ls

